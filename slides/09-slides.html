<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Model-based clustering</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.20/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Model-based clustering
]
.subtitle[
## Gaussian mixture models
]
.date[
### June 15th, 2023
]

---






## Previously...

- We explored the use of __K-means__ and __hierarchical clustering__ for clustering

- These methods yield __hard__ assignments, strictly assigning observations to only one cluster


--
- What about __soft__ assignments? Allow for some __uncertainty__ in the clustering results

--
- Welcome to the wonderful world of __mixture models__


&lt;img src="https://imgs.xkcd.com/comics/t_distribution_2x.png" width="60%" style="display: block; margin: auto;" /&gt;


---

## Previously in kernel density estimation...

$$
\text{Kernel density estimate: } \hat{f}(x) = \frac{1}{n} \sum_{i=1}^n \frac{1}{h} K_h(x - x_i)
$$


--
- We have to use __every observation__ when estimating the density for new points

&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Comparison_of_1D_histogram_and_KDE.png/1000px-Comparison_of_1D_histogram_and_KDE.png" width="60%" style="display: block; margin: auto;" /&gt;


--
- Instead we can make __assumptions__ to "simplify" the problem

---

## Mixture models

We assume the distribution `\(f(x)\)` is a __mixture__ of `\(K\)` __component__ distributions:

`$$f(x) = \sum_{k=1}^K \pi_k f_k(x)$$`

--
- `\(\pi_k =\)` __mixing proportions (or weights)__, where `\(\pi_k &gt; 0\)`, and `\(\sum_k \pi_k = 1\)`


--
This is a __data generating process__, meaning to generate a new point:

1. __pick a distribution / component__ among our `\(K\)` options, by introducing a new variable: 

  - `\(z \sim \text{Multinomial} (\pi_1, \pi_2, \dots, \pi_k)\)`, i.e. categorical variable saying which group the new point is from


--
2. __generate an observation with that distribution / component__, i.e. `\(x | z \sim f_{z}\)`


--
_So what do we use for each `\(f_k\)`?_


---

## Gaussian mixture models (GMMs)

Assume a __parametric mixture model__, with __parameters__ `\(\theta_k\)` for the `\(k\)`th component

`$$f(x) = \sum_{k=1}^K \pi_k f_k(x; \theta_k)$$`


--
Assume each component is [Gaussian / Normal](https://en.wikipedia.org/wiki/Normal_distribution) where for 1D case:

`$$f_k(x; \theta_k) = N(x; \mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi \sigma_k^2}}\text{exp} \Big( -\frac{(x - \mu_k)^2}{2 \sigma_k^2} \Big)$$`

--
We need to estimate each `\(\pi_1, \dots, \pi_k\)`, `\(\mu_1, \dots, \mu_k\)`, `\(\sigma_1, \dots, \sigma_k\)`!


---

## Let's pretend we only have one component...

If we have `\(n\)` observations from a single Normal distribution, we estimate the distribution parameters using the __likelihood function__, the probability / density of observing the data given the parameters

`$$\mathcal{L}(\mu, \sigma | x_1, \dots, x_n) = f( x_1, \dots, x_n | \mu, \sigma) =  \prod_i^n \frac{1}{\sqrt{2\pi \sigma^2}}\text{exp }-\frac{(x_i - \mu)^2}{2 \sigma^2}$$`


--
We can compute the __maximum likelihood estimates (MLEs)__ for `\(\mu\)` and `\(\sigma\)`

__You already know these values!__

- `\(\hat{\mu}_{MLE} = \frac{1}{n} \sum_i^n x_i\)`, sample mean

- `\(\hat{\sigma}_{MLE} = \sqrt{\frac{1}{n}\sum_i^n (x_i - \mu)^2}\)`, sample standard deviation (plug in `\(\hat{\mu}_{MLE}\)`)

---

## The problem with more than one component...

.pull-left[

- __We don't know which component an observation belongs to__

- __IF WE DID KNOW__, then we could compute each component's MLEs as before

- But we don't know because `\(z\)` is a __latent variable__! So what about its distribution given the data?

`$$P(z_i = k | x_i) = \frac{P(x_i | z_i = k) P(z_i = k)}{P(x_i)}$$`

`$$=\frac{\pi_{k} N\left(\mu_{k}, \sigma_{k}^{2}\right)}{\sum_{k=1}^{K} \pi_{k} N\left(\mu_{k}, \sigma_{k}\right)}$$`

- __But we do NOT know these parameters!__

- This leads to a very useful algorithm in statistics...

]

.pull-right[
&lt;img src="09-slides_files/figure-html/init-sim-data-1.png" width="504" style="display: block; margin: auto;" /&gt;
]


---

## Expectation-maximization (EM) algorithm

We alternate between the following:

- _pretending_ to know the probability each observation belongs to each group, to estimate the parameters of the components


--
- _pretending_ to know the parameters of the components, to estimate the probability each observation belong to each group


--
__Where have you seen this before?__

--
K-means algorithm!


--
1. Start with initial guesses about `\(\pi_1, \dots, \pi_k\)`, `\(\mu_1, \dots, \mu_k\)`, `\(\sigma_1, \dots, \sigma_k\)`

2. Repeat until nothing changes:


--
- __Expectation__ step: calculate `\(\hat{z}_{ik}\)` = expected membership of observation `\(i\)` in cluster `\(k\)`

- __Maximization__ step: update parameter estimates with __weighted__ MLE using `\(\hat{z}_{ik}\)`

---

## How does this relate back to clustering?

--
From the EM algorithm:  `\(\hat{z}_{ik}\)` is a __soft membership__ of observation `\(i\)` in cluster `\(k\)`

--
  - you can assign observation `\(i\)` to a cluster with the largest `\(\hat{z}_{ik}\)`
  
  - measure cluster assignment __uncertainty__ `\(= 1 - \text{max}_k \hat{z}_{ik}\)`


--

__Our parameters determine the type of clusters__


--
In 1D we only have two options:


--
1. each cluster __is assumed to have equal variance__ (spread): `\(\sigma_1^2 = \sigma_2^2 = \dots = \sigma_k^2\)`


--
2. each cluster __is allowed to have a different variance__


--
_But that is only 1D... what happens in multiple dimensions?_


---

## Multivariate GMMs

`$$f(x) = \sum_{k=1}^K \pi_k f_k(x; \theta_k) \\ \text{where }f_k(x; \theta_k) \sim N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$`


Each component is a __multivariate normal distribution__:


--
- `\(\boldsymbol{\mu}_k\)` is a _vector_ of means in `\(p\)` dimensions


--
- `\(\boldsymbol{\Sigma}_k\)` is the `\(p \times p\)` __covariance__ matrix - describes the joint variability between pairs of variables

`$$\sum=\left[\begin{array}{cccc}
\sigma_{1}^{2} &amp; \sigma_{1,2} &amp; \cdots &amp; \sigma_{1, p} \\
\sigma_{2,1} &amp; \sigma_{2}^{2} &amp; \cdots &amp; \sigma_{2, p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{p, 1} &amp; \sigma_{p, 2}^{2} &amp; \cdots &amp; \sigma_{p}^{2}
\end{array}\right]$$`

---

## Covariance constraints

`$$\sum=\left[\begin{array}{cccc}
\sigma_{1}^{2} &amp; \sigma_{1,2} &amp; \cdots &amp; \sigma_{1, p} \\
\sigma_{2,1} &amp; \sigma_{2}^{2} &amp; \cdots &amp; \sigma_{2, p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{p, 1} &amp; \sigma_{p, 2}^{2} &amp; \cdots &amp; \sigma_{p}^{2}
\end{array}\right]$$`

--

As we increase the number of dimensions, model fitting and estimation becomes increasingly difficult

--

We can use __constraints__ on multiple aspects of the `\(k\)` covariance matrices:


--
- __volume__: size of the clusters, i.e., number of observations, 

- __shape__: direction of variance, i.e. which variables display more variance

- __orientation__: aligned with axes (low covariance) versus tilted (due to relationships between variables)

---

&lt;img src="files/figures/gmms_parameter-visual.jpg" width="70%" style="display: block; margin: auto;" /&gt;

- Control volume, shape, orientation

- __E__ means equal and __V__ means variable (_VVV_ is the most flexible, but has the most parameters)

- Two II is __spherical__, one I is __diagonal__, and the remaining are __general__

---

## So many options! How do we know what to do?

&lt;img src="https://i.pinimg.com/originals/c8/d5/0e/c8d50e648631eb60535305d2da3ceb2a.gif" width="70%" style="display: block; margin: auto;" /&gt;

---

## Bayesian information criterion (BIC)

__This is a statistical model__

`$$f(x) = \sum_{k=1}^K \pi_k f_k(x; \theta_k) \\ \text{where }f_k(x; \theta_k) \sim N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$`


--
Meaning we can use a __model selection__ procedure for determining which best characterizes the data


--
Specifically - we will use a __penalized likelihood__ measure

`$$BIC = 2\log \mathcal{L} - m\log n$$`

- `\(\log \mathcal{L}\)` is the log-likelihood of the considered model

- with `\(m\)` parameters (_VVV_ has the most parameters) and `\(n\)` observations


--
- __penalizes__ large models with __many clusters without constraints__

- __we can use BIC to choose the covariance constraints AND number of clusters `\(K\)`!__

_The above BIC is really the -BIC of what you typically see, this sign flip is just for ease_


---

### Mixture model for NBA players... New dataset!

Created dataset of NBA player statistics per 100 possessions using [`ballr`](https://cran.r-project.org/web/packages/ballr/vignettes/use-ballr.html)


```r
library(tidyverse)
nba_pos_stats &lt;- 
  read_csv("https://shorturl.at/mFGY2")
# Find rows for players indicating a full season worth of stats
tot_players &lt;- nba_pos_stats %&gt;% filter(tm == "TOT")
# Stack this dataset with players that played on just one team
nba_player_stats &lt;- nba_pos_stats %&gt;% 
  filter(!(player %in% tot_players$player)) %&gt;% 
  bind_rows(tot_players)
# Filter to only players with at least 125 minutes played
nba_filtered_stats &lt;- nba_player_stats %&gt;% filter(mp &gt;= 125)
head(nba_filtered_stats)
```

```
## # A tibble: 6 × 31
##   player     pos     age tm        g    gs    mp    fg   fga fgper…¹   x3p  x3pa
##   &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Precious … C        22 TOR      73    28  1725   7.7  17.5   0.439   1.6   4.5
## 2 Steven Ad… C        28 MEM      76    75  1999   5     9.2   0.547   0     0  
## 3 Bam Adeba… C        24 MIA      56    56  1825  11.1  20     0.557   0     0.2
## 4 Santi Ald… PF       21 MEM      32     0   360   7    17.5   0.402   0.8   6.4
## 5 LaMarcus … C        36 BRK      47    12  1050  11.6  21.1   0.55    0.6   2.1
## 6 Grayson A… SG       26 MIL      66    61  1805   6.8  15.1   0.448   4.2  10.4
## # … with 19 more variables: x3ppercent &lt;dbl&gt;, x2p &lt;dbl&gt;, x2pa &lt;dbl&gt;,
## #   x2ppercent &lt;dbl&gt;, ft &lt;dbl&gt;, fta &lt;dbl&gt;, ftpercent &lt;dbl&gt;, orb &lt;dbl&gt;,
## #   drb &lt;dbl&gt;, trb &lt;dbl&gt;, ast &lt;dbl&gt;, stl &lt;dbl&gt;, blk &lt;dbl&gt;, tov &lt;dbl&gt;, pf &lt;dbl&gt;,
## #   pts &lt;dbl&gt;, x &lt;lgl&gt;, ortg &lt;dbl&gt;, drtg &lt;dbl&gt;, and abbreviated variable name
## #   ¹​fgpercent
```

---

## Gaussian Mixture Models with [`mclust`](https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html)

Use the `Mclust` function to search over 1 to 9 clusters (_K_ = `G`) and the different covariance constraints (i.e. models) 


```r
library(mclust)
nba_mclust &lt;- Mclust(dplyr::select(nba_filtered_stats, x3pa, trb))
```



We can use the `summary()` function to display the selection and resulting table of assignments:


```r
summary(nba_mclust)
```

```
## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust VVI (diagonal, varying volume and shape) model with 3 components: 
## 
##  log-likelihood   n df       BIC       ICL
##        -2459.03 483 14 -5004.581 -5141.138
## 
## Clustering table:
##   1   2   3 
##  52 276 155
```


---

## Display the BIC for each model and number of clusters

.pull-left[

```r
plot(nba_mclust, what = 'BIC', 
     legendArgs = list(x = "bottomright", 
                       ncol = 4))
```

&lt;img src="09-slides_files/figure-html/nba-bic-1.png" width="504" style="display: block; margin: auto;" /&gt;
]
.pull-right[

```r
plot(nba_mclust, what = 'classification')
```

&lt;img src="09-slides_files/figure-html/nba-cluster-plot-1.png" width="504" /&gt;
]

---

## How do the cluster assignments compare to the positions?

We can again compare the clustering assignments with player positions:


```r
table("Clusters" = nba_mclust$classification, "Positions" = nba_filtered_stats$pos)
```

```
##         Positions
## Clusters  C C-PF PF PF-SF PG PG-SG SF SF-SG SG SG-PG SG-SF
##        1 43    0  9     0  0     0  0     0  0     0     0
##        2  3    0 28     0 84     0 54     5 96     3     3
##        3 39    2 56     1  8     1 38     0  9     0     1
```

---

## What about the cluster probabilities?

.pull-left[

```r
*nba_player_probs &lt;- nba_mclust$z
colnames(nba_player_probs) &lt;- 
  paste0('Cluster ', 1:3)

nba_player_probs &lt;- nba_player_probs %&gt;%
  as_tibble() %&gt;%
  mutate(player = 
           nba_filtered_stats$player) %&gt;%
* pivot_longer(contains("Cluster"),
*              names_to = "cluster",
*              values_to = "prob")

nba_player_probs %&gt;%
  ggplot(aes(prob)) +
  geom_histogram() +
  theme_bw() +
  facet_wrap(~ cluster, nrow = 2)
```
]
.pull-right[
&lt;img src="09-slides_files/figure-html/unnamed-chunk-6-1.png" width="504" /&gt;
]

---

## Which players have the highest uncertainty?

.pull-left[

```r
nba_filtered_stats %&gt;%
* mutate(cluster =
*          nba_mclust$classification,
*        uncertainty =
*          nba_mclust$uncertainty) %&gt;%
  group_by(cluster) %&gt;%
  arrange(desc(uncertainty)) %&gt;%
  slice(1:5) %&gt;%
  ggplot(aes(y = uncertainty, 
*            x = reorder(player,
*                        uncertainty))) +
  geom_point() +
  coord_flip() + 
  theme_bw() +
  facet_wrap(~ cluster, 
             scales = 'free_y', nrow = 3)
```
]
.pull-right[
&lt;img src="09-slides_files/figure-html/unnamed-chunk-7-1.png" width="504" /&gt;
]


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
