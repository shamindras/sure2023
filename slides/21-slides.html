<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Machine learning</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.20/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Machine learning
]
.subtitle[
## Random forests and gradient-boosted trees
]
.date[
### July 7th, 2023
]

---







## Decision trees review

Decision trees partition training data into __homogenous nodes / subgroups__ with similar response values

__Pros__

- Decision trees are __very easy to explain__ to non-statisticians.

- Easy to visualize and thus easy to interpret __without assuming a parametric form__

--

__Cons__


- High variance, i.e. split a dataset in half and grow tress in each half, the result will be very different

- Related note - __they generalize poorly resulting in higher test set error rates__

--

But there are several ways we can overcome this via __ensemble models__

---

## Bagging

__Bootstrap aggregation__ (aka bagging) is a general approach for overcoming high variance


--
- __Bootstrap__: sample the training data _with replacement_

&lt;img src="https://bradleyboehmke.github.io/HOML/images/bootstrap-scheme.png" width="60%" style="display: block; margin: auto;" /&gt;


--
- __Aggregation__: Combine the results from many trees together, each constructed with a different bootstrapped sample of the data


---

## Bagging algorithm

Start with a __specified number of trees `\(B\)`__:


--
- For each tree `\(b\)` in `\(1, \dots, B\)`:

  - Construct a bootstrap sample from the training data
  
  
--
  - Grow a deep, unpruned, complicated (aka really overfit!) tree


--
To generate a prediction for a new point:

- __Regression__: take the __average__ across the `\(B\)` trees


--
- __Classification__: take the __majority vote__ across the `\(B\)` trees 

  - assuming each tree predicts a single class (could use probabilities instead...)



--
Improves prediction accuracy via __wisdom of the crowds__ - but at the expense of interpretability

- Easy to read one tree, but how do you read `\(B = 500\)`?


--
But we can still use the measures of __variable importance__ and __partial dependence__ to summarize our models

---

## Random forests algorithm

Random forests are __an extension of bagging__


--
- For each tree `\(b\)` in `\(1, \dots, B\)`:

  - Construct a bootstrap sample from the training data
  
  
--
  - Grow a deep, unpruned, complicated (aka really overfit!) tree __but with a twist__
  
  
--
  - __At each split__: limit the variables considered to a __random subset__ `\(m_{try}\)` of original `\(p\)` variables


--
Predictions are made the same way as bagging:

- __Regression__: take the __average__ across the `\(B\)` trees


- __Classification__: take the __majority vote__ across the `\(B\)` trees 


--
__Split-variable randomization__ adds more randomness to make __each tree more independent of each other__ 


--
Introduce `\(m_{try}\)` as a tuning parameter: typically use `\(p / 3\)` (regression) or `\(\sqrt{p}\)` (classification)
- `\(m_{try} = p\)`
--
 is bagging

---

## Example data: MLB 2022 batting statistics

Downloaded MLB 2022 batting statistics leaderboard from [Fangraphs](https://www.fangraphs.com/leaders.aspx?pos=all&amp;stats=bat&amp;lg=all&amp;qual=y&amp;type=8&amp;season=2022&amp;month=0&amp;season1=2022&amp;ind=0)


```r
library(tidyverse)
mlb_data &lt;- read_csv("https://shorturl.at/iCP15") %&gt;%
  janitor::clean_names() %&gt;%
  mutate_at(vars(bb_percent:k_percent), parse_number)
model_mlb_data &lt;- mlb_data %&gt;%
  dplyr::select(-name, -team, -playerid)
head(model_mlb_data)
```

```
## # A tibble: 6 × 20
##       g    pa    hr     r   rbi    sb bb_percent k_percent   iso babip   avg
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1    85   374    22    62    55     2        6.7      17.6 0.28  0.353 0.327
## 2    88   385    33    72    69     8       11.4      26   0.337 0.296 0.281
## 3    88   370    18    41    59     1        8.9      13   0.233 0.295 0.293
## 4    82   349    15    56    51     7       10.6      18.6 0.213 0.346 0.306
## 5    90   391    20    64    70     5       12        21.2 0.26  0.388 0.33 
## 6    87   375    19    54    75    13       10.7       9.9 0.288 0.275 0.288
## # ℹ 9 more variables: obp &lt;dbl&gt;, slg &lt;dbl&gt;, w_oba &lt;dbl&gt;, xw_oba &lt;dbl&gt;,
## #   w_rc &lt;dbl&gt;, bs_r &lt;dbl&gt;, off &lt;dbl&gt;, def &lt;dbl&gt;, war &lt;dbl&gt;
```


---

## Example using [`ranger`](https://github.com/imbs-hl/ranger)

`ranger` package is a popular / fast implementation ([see `randomForest`](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) for the original)


```r
library(ranger)
init_mlb_rf &lt;- ranger(war ~ ., data = model_mlb_data, num.trees = 50, importance = "impurity")
init_mlb_rf
```

```
## Ranger result
## 
## Call:
##  ranger(war ~ ., data = model_mlb_data, num.trees = 50, importance = "impurity") 
## 
## Type:                             Regression 
## Number of trees:                  50 
## Sample size:                      157 
## Number of independent variables:  19 
## Mtry:                             4 
## Target node size:                 5 
## Variable importance mode:         impurity 
## Splitrule:                        variance 
## OOB prediction error (MSE):       0.2365584 
## R squared (OOB):                  0.8433193
```

---

## Out-of-bag estimate

Since the trees are constructed via bootstrapped data (samples with replacements) - each sample _is likely to have duplicate observations / rows_

__Out-of-bag (OOB)__ - original observations not contained in a single bootstrap sample

- Can use the OOB samples to estimate predictive performance (OOB becomes better with larger datasets)

- On average `\(\approx 63\)`% of original data ends up in any particular bootstrap sample

---

## Variable importance


```r
library(vip)
vip(init_mlb_rf, geom = "point") + theme_bw()
```

&lt;img src="21-slides_files/figure-html/unnamed-chunk-3-1.png" width="504" style="display: block; margin: auto;" /&gt;

---

## Tuning random forests

Unfortunately `caret` does not let you know tune number of trees - typically the error goes down with more (_Exercise: check out CV performance as a function of the number trees on your own, compare with OOB error_)

.pull-left[
- __Important__: `\(m_{try}\)`

- Marginal: tree complexity, splitting rule, sampling scheme



```r
library(caret)
rf_tune_grid &lt;- 
  expand.grid(mtry = seq(3, 18, by = 3), 
              splitrule = "variance",
              min.node.size = 5)
set.seed(1917)
caret_mlb_rf &lt;- 
  train(war ~ ., data = model_mlb_data,
        method = "ranger", num.trees = 50,
        trControl = trainControl(
          method = "cv", number = 5),
        tuneGrid = rf_tune_grid)
```

]
.pull-right[

```r
ggplot(caret_mlb_rf) + theme_bw()
```

&lt;img src="21-slides_files/figure-html/unnamed-chunk-5-1.png" width="504" style="display: block; margin: auto;" /&gt;

]

---

## Boosting

Build ensemble models __sequentially__


--
- start with a __weak learner__, e.g. small decision tree with few splits


--
- each model in the sequence _slightly_ improves upon the predictions of the previous models __by focusing on the observations with the largest errors / residuals__

&lt;img src="https://bradleyboehmke.github.io/HOML/images/boosted-trees-process.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Boosted trees algorithm

Write the prediction at step `\(t\)` of the search as `\(\hat{y}_i^{(t)}\)`, start with `\(\hat{y}_i^{(0)} = 0\)`

- Fit the first decision tree `\(f_1\)` to the data: `\(\hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\)`


--
- Fit the next tree `\(f_2\)` to the residuals of the previous: `\(y_i - \hat{y}_i^{(1)}\)`

- Add this to the prediction: `\(\hat{y}_i^{(2)} = \hat{y}_i^{(1)} + f_2(x_i) = f_1(x_i) + f_2(x_i)\)`


--
- Fit the next tree `\(f_3\)` to the residuals of the previous: `\(y_i - \hat{y}_i^{(2)}\)`

- Add this to the prediction:  `\(\hat{y}_i^{(3)} = \hat{y}_i^{(2)} + f_3(x_i) = f_1(x_i) + f_2(x_i) + f_3(x_i)\)`


--
__Continue until some stopping criteria__ to reach final model as a __sum of trees__:

`$$\hat{y_i} = f(x_i) = \sum_{b=1}^B f_b(x_i)$$`

---

## Visual example of boosting in action

&lt;img src="https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/boosting-in-action-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---

## Gradient boosted trees

Regression boosting algorithm can be generalized to other loss functions via __gradient descent__ - leading to gradient boosted trees, aka __gradient boosting machines (GBMs)__

Update the model parameters in the direction of the loss function's descending gradient 

&lt;img src="https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/gradient-descent-fig-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---

## Tune the learning rate in gradient descent

We need to control how much we update by in each step - __the learning rate__

&lt;img src="https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/learning-rate-fig-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

### Stochastic gradient descent can help with complex loss functions

Can take random samples of the data when updating - makes algorithm faster and adds randomness to get closer to global minimum (no guarantees!)

&lt;img src="https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/stochastic-gradient-descent-fig-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## eXtreme gradient boosting with [XGBoost](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html)

&lt;img src="https://media1.tenor.com/images/6e86cad4899cd02047c33ecfc0e4052b/tenor.gif?itemid=11446810" width="80%" style="display: block; margin: auto;" /&gt;


---

## Tuning GBMs with [`xgboost`](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html)

__XGBoost__ (extreme gradient boosting) is a very powerful, efficient boosting library that is available to use within `R` via the [`xgboost`](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html) package


--
What we have to consider tuning (our __hyperparameters__):

- number of trees `\(B\)` (`nrounds`)

- learning rate (`eta`), i.e. how much we update in each step

- these two really have to be tuned together

--
- complexity of the trees (depth, number of observations in nodes)


--
- XGBoost also provides more __regularization__ (via `gamma`) and early stopping


--
__More work to tune properly as compared to random forests__

- But GBMs have more flexibility in their usage for particular objective functions

- _Insert with great power comes great responsibility meme_

---

## XGBoost example


```r
library(xgboost)
xgboost_tune_grid &lt;- expand.grid(nrounds = seq(from = 20, to = 200, by = 20),
                                 eta = c(0.025, 0.05, 0.1, 0.3), gamma = 0,
                                 max_depth = c(1, 2, 3, 4), colsample_bytree = 1,
                                 min_child_weight = 1, subsample = 1)
xgboost_tune_control &lt;- trainControl(method = "cv", number = 5, verboseIter = FALSE)
set.seed(1937)
xgb_tune &lt;- train(x = as.matrix(dplyr::select(model_mlb_data, -war)),
                  y = model_mlb_data$war, trControl = xgboost_tune_control,
                  tuneGrid = xgboost_tune_grid, 
                  objective = "reg:squarederror", method = "xgbTree",
                  verbosity = 0)
xgb_tune$bestTune
```

```
##     nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 130     200         1 0.3     0                1                1         1
```

---

## XGBoost example


```r
xgb_fit_final &lt;- xgboost(data = as.matrix(dplyr::select(model_mlb_data, -war)),
                         label = model_mlb_data$war, objective = "reg:squarederror",
                         nrounds = xgb_tune$bestTune$nrounds,
                         params = as.list(dplyr::select(xgb_tune$bestTune,
                                                        -nrounds)), 
                         verbose = 0)
vip(xgb_fit_final) + theme_bw()
```

&lt;img src="21-slides_files/figure-html/unnamed-chunk-13-1.png" width="504" style="display: block; margin: auto;" /&gt;


---

## XGBoost example


```r
library(pdp)
partial(xgb_fit_final, pred.var = "off", train = as.matrix(dplyr::select(model_mlb_data, -war)),
        plot.engine = "ggplot2", plot = TRUE,
        type = "regression") + theme_bw()
```

&lt;img src="21-slides_files/figure-html/unnamed-chunk-14-1.png" width="504" style="display: block; margin: auto;" /&gt;



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
