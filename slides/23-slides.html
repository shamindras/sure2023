<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Advanced Topics in Regression</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.20/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Advanced Topics in Regression
]
.subtitle[
## Kernels, Smoothers, and Generalized Additive Models
]
.date[
### July 11th, 2023
]

---






## Model flexibility vs interpretability

[Figure 2.7, Introduction to Statistical Learning with Applications in R (ISLR)](https://www.statlearning.com/)

&lt;img src="http://www.stat.cmu.edu/~pfreeman/flexibility.png" width="50%" style="display: block; margin: auto;" /&gt;

__Tradeoff__ between model's _flexibility_ (i.e. how "curvy" it is) and how __interpretable__ it is

- Simpler, parametric form of the model `\(\Rightarrow\)` the easier it is to interpret

---

## Model flexibility vs interpretability

&lt;img src="http://www.stat.cmu.edu/~pfreeman/flexibility.png" width="50%" style="display: block; margin: auto;" /&gt;

- __Parametric__ models, for which we can write down a mathematical expression for `\(f(X)\)` __before observing the data__, _a priori_ (e.g. linear regression), __are inherently less flexible__


--

- __Nonparametric__ models, in which `\(f(X)\)` is __estimated from the data__ (e.g. kernel regression)

---

## Recall: K Nearest Neighbors (KNN)

- Find the `\(k\)` data points __closest__ to an observation `\(x\)`, use these to predict

  - Regression: `\({\hat Y} \vert X = \frac{1}{k} \sum_{i=1}^k Y_i\)` (_average response_)

  - Classification: `\(\hat{P}[Y = j \vert X] = \frac{1}{k} \sum_{i=1}^k 1(Y_i = j)\)` (_majority vote_)


--

Determining the optimal value of `\(k\)` requires balancing bias and variance

&lt;img src="http://www.stat.cmu.edu/~pfreeman/Fig_2.16.png" width="40%" style="display: block; margin: auto;" /&gt;

---

## Averaging with Neighbors?? Kernels!!

A kernel `\(K(x)\)` is a weighting function used in estimators, and technically has only one required property:

- `\(K(x) \geq 0\)` for all `\(x\)`

However, in the manner that kernels are used in statistics, there are two other properties that are usually satisfied:

- `\(\int_{-\infty}^\infty K(x) dx = 1\)`; and 

- `\(K(-x) = K(x)\)` for all `\(x\)`.

In short: __a kernel is a symmetric PDF!__

---

## Recall: Kernel density estimation

__Goal__: estimate the PDF `\(f(x)\)` for all possible values (assuming it is continuous / smooth)


$$
\text{Kernel density estimate: } \hat{f}(x) = \frac{1}{n} \sum_{i=1}^n \frac{1}{h} K_h(x - x_i)
$$

- `\(n =\)` sample size, `\(x =\)` new point to estimate `\(f(x)\)` (does NOT have to be in dataset!)

- `\(h =\)` __bandwidth__, analogous to histogram bin width, ensures `\(\hat{f}(x)\)` integrates to 1

- `\(x_i =\)` `\(i\)`th observation in dataset

- `\(K_h(x - x_i)\)` is the __Kernel__ function, creates __weight__ given distance of `\(i\)`th observation from new point 
  - as `\(|x - x_i| \rightarrow \infty\)` then `\(K_h(x - x_i) \rightarrow 0\)`, i.e. further apart `\(i\)`th row is from `\(x\)`, smaller the weight
  
  - as __bandwidth__ `\(h \uparrow\)` weights are more evenly spread out (as `\(h \downarrow\)` more concentrated around `\(x\)`) 

  - typically use [__Gaussian__ / Normal](https://en.wikipedia.org/wiki/Normal_distribution) kernel: `\(\propto e^{-(x - x_i)^2 / 2h^2}\)`
  
  - `\(K_h(x - x_i)\)` is large when `\(x_i\)` is close to `\(x\)`
  

---

## Commonly Used Kernels

&lt;img src="http://www.stat.cmu.edu/~pfreeman/kernels.png" width="40%" style="display: block; margin: auto;" /&gt;


A general rule of thumb: the choice of kernel will have little effect on estimation, particularly if the sample size is large! The Gaussian kernel (i.e., a normal PDF) is by far the most common choice, and is the default for `R` functions that utilize kernels.


---

## Kernel regression

We can apply kernels in the regression setting as well as in the density estimation setting!

The classic kernel regression estimator is the __Nadaraya-Watson__ estimator:

`$$\hat{y}_h(x) = \sum_{i=1}^n w_i(x) Y_i \,,$$`

where
`$$w_i(x) = \frac{K\left(\frac{x-X_i}{h}\right)}{\sum_{j=1}^n K\left(\frac{x-X_j}{h}\right)} \,.$$`

Regression estimate is the average of all the *weighted* observed response values; 

- Farther `\(x\)` is from observation `\(\Rightarrow\)` less weight that observation has in determining the regression estimate at `\(x\)`

---

## Kernel regression

__Nadaraya-Watson kernel regression__

- given training data with explanatory variable `\(x\)` and continuous response `\(y\)` 

- _bandwidth_ `\(h &gt; 0\)`

- and a new point `\((x_{new}, y_{new})\)`: 

`$$\hat{y}_{new} = \sum_{i=1}^n w_i(x_{new})\cdot y_i \,,$$`

where

`$$w_i(x) = \frac{K_h\left(|x_{new}-x_i|\right)}{\sum_{j=1}^n K_h\left(|x_{new}-x_j|\right)} \text{ with } K_h(x) = K(\frac{x}{h})$$`

Example of a __linear smoother__

- class of models where predictions are _weighted_ sums of the response variable

---

## Local regression

We can fit a linear model __at each point `\(x_{new}\)`__ with weights given by kernel function centered on `\(x_{new}\)`

- we can additionally combine this with _polynomial regression_

--

Local regression of the `\(k^{th}\)` order with kernel function `\(K\)` solves the following:

`$$\hat{\beta}(x_{new}) = \underset{\beta}{\text{arg min}}\Big\{ \sum_i K_h(|x_{new} - x_i|) \cdot (y_i - \sum_{j=0}^k x_i^k \cdot \beta_k )^2 \Big\}$$`

--
__Yes, this means every single observation has its own set of coefficients__


--

Predicted value is then:

`$$\hat{y}_{new} = \sum_{j=0}^k x_{new}^k \cdot \hat{\beta}_k(x_{new})$$`


--

Smoother predictions than kernel regression but comes at __higher computational cost__ 

- __LOESS__ replaces kernel with k nearest neighbors
  
  - faster than local regression but discontinuities when neighbors change


---

## Smoothing splines


Use __smooth function__ `\(s(x)\)` to predict `\(y\)`, control smoothness directly by minimizing the __spline objective function__:

`$$\sum_{i=1}^n (y_i - s(x_i))^2 + \lambda \int(s''(x))^2dx$$`


--

`$$= \text{fit data} + \text{impose smoothness}$$`

--

`$$\Rightarrow \text{model fit} = \text{likelihood} - \lambda \cdot \text{wiggliness}$$`
Estimate the __smoothing spline__ `\(\hat{s}(x)\)` that __balances the tradeoff between the model fit and wiggliness__


--
&lt;img src="https://github.com/noamross/gams-in-r-course/blob/master/images/diffsmooth-1.png?raw=true" width="65%" style="display: block; margin: auto;" /&gt;

---

## Basis functions

Splines are _piecewise cubic polynomials_ with __knots__ (boundary points for functions) at every data point


--

Practical alternative: linear combination of set of __basis functions__


--
__Cubic polynomial example__: define four basis functions:

- `\(B_1(x) = 1\)`, `\(B_2(x) = x\)`, `\(B_3(x) = x^2\)`, `\(B_4(x) = x^3\)`

where the regression function `\(r(x)\)` is written as:

`$$r(x) = \sum_j^4 \beta_j B_j(x)$$`


--
- __linear in the transformed variables__ `\(B_1(x), \dots, B_4(x)\)` but it is __nonlinear in `\(x\)`__


--
We extend this idea for splines _piecewise_ using indicator functions so the spline is a weighted sum:

`$$s(x) = \sum_j^m \beta_j B_j(x)$$`

---

## Number of basis functions is another tuning parameter

&lt;img src="https://github.com/noamross/gams-in-r-course/blob/master/images/diffbasis-1.png?raw=true" width="80%" style="display: block; margin: auto;" /&gt;



---

## Generalized additive models (GAMs)

GAMs were created by [Trevor Hastie and Rob Tibshirani in 1986](https://projecteuclid.org/euclid.ss/1177013604) with intuitive construction:

- relationships between individual explanatory variables and the response variable are smooth (either linear or nonlinear via basis functions)

- estimate the smooth relationships __simultaneously__ to predict the response by just adding them up

__Generalized__ like GLMs where `\(g()\)` is the link function for the expected value of the response `\(E(Y)\)` and __additive__ over the `\(p\)` variables:

`$$g(E(Y)) = \beta_0 + s_1(x_1) + s_2(x_2) + \dots + s_p(x_p)$$`

&lt;img src="https://multithreaded.stitchfix.com/assets/images/blog/fig1.svg" width="70%" style="display: block; margin: auto;" /&gt;


--

- can be a convenient balance between flexibility and interpretability

- you can combine linear and nonlinear terms!

---

## Example: predicting MLB HR probability

Used the [`baseballr`](http://billpetti.github.io/baseballr/) package to scrape all batted-balls from 2022 season:


```r
library(tidyverse)
batted_ball_data &lt;- read_csv("https://shorturl.at/moty2") %&gt;%
  mutate(is_hr = as.numeric(events == "home_run")) %&gt;%
  filter(!is.na(launch_angle), !is.na(launch_speed),
         !is.na(is_hr))
head(batted_ball_data)
```

```
## # A tibble: 6 × 32
##   player_name    batter stand events     hc_x  hc_y hit_distance_sc launch_speed
##   &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;
## 1 Daza, Yonathan 602074 R     force_out 103.  150.               18         97.4
## 2 Robles, Victor 645302 R     single     58.6 120.              158         80.2
## 3 Hoerner, Nico  663538 R     field_out  99.3 166.               20        101. 
## 4 Clemens, Kody  665019 L     field_out 126.  191.              165         84  
## 5 Rosario, Amed  642708 R     field_out  97.4 170.                9         94.3
## 6 Castro, Willi  650489 L     sac_fly   178.   58.9             369         96  
## # ℹ 24 more variables: launch_angle &lt;dbl&gt;, hit_location &lt;dbl&gt;, bb_type &lt;chr&gt;,
## #   barrel &lt;dbl&gt;, pitch_type &lt;chr&gt;, release_speed &lt;dbl&gt;, effective_speed &lt;dbl&gt;,
## #   if_fielding_alignment &lt;chr&gt;, of_fielding_alignment &lt;chr&gt;, game_date &lt;date&gt;,
## #   balls &lt;dbl&gt;, strikes &lt;dbl&gt;, outs_when_up &lt;dbl&gt;, on_1b &lt;dbl&gt;, on_2b &lt;dbl&gt;,
## #   on_3b &lt;dbl&gt;, inning &lt;dbl&gt;, inning_topbot &lt;chr&gt;, home_score &lt;dbl&gt;,
## #   away_score &lt;dbl&gt;, post_home_score &lt;dbl&gt;, post_away_score &lt;dbl&gt;, des &lt;chr&gt;,
## #   is_hr &lt;dbl&gt;
```

```r
table(batted_ball_data$is_hr)
```

```
## 
##    0    1 
## 6702  333
```

---

## Predict HRs with launch angle and exit velocity?

.pull-left[

```r
batted_ball_data %&gt;%
  ggplot(aes(x = launch_speed, 
             y = launch_angle,
             color = as.factor(is_hr))) +
  geom_point(alpha = 0.5) +
  ggthemes::scale_color_colorblind(
    labels = c("No", "Yes")) +
  labs(x = "Exit velocity", 
       y = "Launch angle", 
       color = "HR?") +
  theme_bw() +
  theme(legend.position = "bottom")
```

- HRs are relatively rare and confined to one area of this plot

]
.pull-right[
&lt;img src="23-slides_files/figure-html/unnamed-chunk-8-1.png" width="504" /&gt;

]

---

## Fitting GAMs with [`mgcv`](https://cran.r-project.org/web/packages/mgcv/index.html)

First set-up training data


```r
set.seed(2004)
batted_ball_data &lt;- batted_ball_data %&gt;%
  mutate(is_train = sample(rep(0:1, length.out = nrow(batted_ball_data))))
```

Next fit the initial function using smooth functions via `s()`:


```r
library(mgcv)
*init_logit_gam &lt;- gam(is_hr ~ s(launch_speed) + s(launch_angle),
                      data = filter(batted_ball_data, is_train == 1), 
*                     family = binomial, method = "REML")
```

 - Use [REML](https://en.wikipedia.org/wiki/Restricted_maximum_likelihood) instead of the default for more stable solution

---

### GAM summary


```r
summary(init_logit_gam)
```

```
## 
## Family: binomial 
## Link function: logit 
## 
## Formula:
## is_hr ~ s(launch_speed) + s(launch_angle)
## 
## Parametric coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   -26.96      10.31  -2.614  0.00895 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##                   edf Ref.df Chi.sq p-value    
## s(launch_speed) 1.000  1.000  151.5  &lt;2e-16 ***
## s(launch_angle) 2.962  3.305  112.0  &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =  0.588   Deviance explained = 68.3%
## -REML = 231.49  Scale est. = 1         n = 3517
```


---

### Visualizing partial response functions with [gratia](https://gavinsimpson.github.io/gratia/index.html)

Displays the partial effect of each term in the model `\(\Rightarrow\)` add up to the overall prediction 


```r
library(gratia)
draw(init_logit_gam)
```

&lt;img src="23-slides_files/figure-html/unnamed-chunk-9-1.png" width="504" style="display: block; margin: auto;" /&gt;

---

## Convert to probability scale with `plogis` function



```r
*draw(init_logit_gam, fun = plogis)
```

&lt;img src="23-slides_files/figure-html/unnamed-chunk-10-1.png" width="504" style="display: block; margin: auto;" /&gt;

--

- centered on average value of 0.5 because it's the partial effect without the intercept

---

## Include intercept in plot...


```r
*draw(init_logit_gam, fun = plogis, constant = coef(init_logit_gam)[1])
```

&lt;img src="23-slides_files/figure-html/unnamed-chunk-11-1.png" width="504" style="display: block; margin: auto;" /&gt;

Intercept reflects relatively rare occurence of HRs!

---

## Model checking for number of basis functions

Use `gam.check()` to see if we need more basis functions based on an approximate test


```r
gam.check(init_logit_gam)
```

```
## 
## Method: REML   Optimizer: outer newton
## full convergence after 11 iterations.
## Gradient range [-5.632542e-05,-2.964163e-06]
## (score 231.4864 &amp; scale 1).
## Hessian positive definite, eigenvalue range [5.631851e-05,0.8679399].
## Model rank =  19 / 19 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k'.
## 
##                   k'  edf k-index p-value  
## s(launch_speed) 9.00 1.00    1.05    1.00  
## s(launch_angle) 9.00 2.96    0.97    0.08 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---

## Check the predictions?


```r
batted_ball_data &lt;- batted_ball_data %&gt;%
  mutate(init_gam_hr_prob = 
           as.numeric(predict(init_logit_gam,
                              newdata = batted_ball_data,
                              type = "response")),
         init_gam_hr_class = as.numeric(init_gam_hr_prob &gt;= 0.5))
batted_ball_data %&gt;%
  group_by(is_train) %&gt;%
  summarize(correct = mean(is_hr == init_gam_hr_class))
```

```
## # A tibble: 2 × 2
##   is_train correct
##      &lt;int&gt;   &lt;dbl&gt;
## 1        0   0.977
## 2        1   0.972
```

---

## What about the linear model?


```r
init_linear_logit &lt;- glm(is_hr ~ launch_speed + launch_angle, 
                         data = filter(batted_ball_data, is_train == 1), 
                         family = binomial)
batted_ball_data &lt;- batted_ball_data %&gt;%
  mutate(init_glm_hr_prob = predict(init_linear_logit,
                                    newdata = batted_ball_data,
                                    type = "response"),
         init_glm_hr_class = as.numeric(init_glm_hr_prob &gt;= 0.5))
batted_ball_data %&gt;%
  group_by(is_train) %&gt;%
  summarize(correct = mean(is_hr == init_glm_hr_class))
```

```
## # A tibble: 2 × 2
##   is_train correct
##      &lt;int&gt;   &lt;dbl&gt;
## 1        0   0.960
## 2        1   0.951
```


__Very few situations in reality where linear regressions perform better than an additive model using smooth functions__ - especially since smooth functions can just capture linear models...

---

## Some useful resources


- [GAMs in R by Noam Ross](https://noamross.github.io/gams-in-r-course/)

- [`mgcv` course](https://eric-pedersen.github.io/mgcv-esa-workshop/)

- [Stitch Fix post: GAM: The Predictive Modeling Silver Bullet](https://multithreaded.stitchfix.com/blog/2015/07/30/gam/)

- Chapters 7 and 8 of [Advanced Data Analysis from an Elementary Point of View by Prof Cosma Shalizi](https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf) 

  - I strongly recommend you download this book, and you will refer back to it for the rest of your life
  


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
