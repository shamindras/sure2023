---
title: "Machine Learning with R"
subtitle: "tidymodels"
date: "June 27th, 2023"
output:
  xaringan::moon_reader:
    lib_dir: "libs"
    # chakra: "libs/remark-latest.min.js"
    # css: ["default", "css/ath-slides.css", "css/ath-inferno-fonts.css", "css/animate.css"]
    self-contained: yes
    # css: [default, default-fonts]
    # seal: false
    # anchor_sections: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      navigation:
        scroll: false
editor_options:
  chunk_output_type: inline
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(tidyverse)
style_mono_accent(base_color = "#cc002b")
```

```{r setup, echo = FALSE}
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
```

## So far...

__Exploratory data analysis__: Wrangling, visualization $\rightarrow$ tidyverse

__Unsupervised learning__: K-means clustering (stats), Kmeans++ (flexclust), Hierarchical clustering (stats), minimax linkage (protoclust), Gaussian mixture models (mclust)

__Linear regression__: simple/multiple linear regression (stats), ridge / lasso / elastic net (glmnet)

--

Within supervised learning we've only really seen two methods: `lm` and `glmnet`, but they've already had differences:

--
- x and y together? separate?
- cross-validation?

---

## R is free, open-source, and user-contributed

__R__ comes out of the __S__ programming language (Bell Labs)... object oriented!

But many (if not most) of the things we do in __R__ come from "packages" rather than from "base R", which includes the `base` and `stats` packages

__Packages__ are created by users:

.pull-left[

__Pro__

- You come up with a method, you can code it right up!

- Then I can use your code to fit a model!

]

.pull-right[

__Con__

- Say what? `G` = `k` = `centers`? nzero?

- Many ways to do one thing may not be ideal

]

---

## Into the tidy~~verse~~ models

.pull-left[

Like the __tidyverse__, __tidymodels__ is a suite of packages dedicated to fitting and using models in a _tidy_ way, enabling interaction with the other packages we've used so far (`dplyr`, `ggplot`, etc.)

- __parsnip__ simplifies model parameters and interfaces

- __recipes__ enables feature engineering and preprocessing

- __workflows__ allow you to store and execute steps together

- __tune__ optimizes hyperparameters

(Also like the tidyverse, tidymodels has fantastic [documentation](https://www.tidymodels.org/))

]

.pull-right[

```{r out.width='90%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://rviews.rstudio.com/2020/04/21/the-case-for-tidymodels/tidymodels.png")
```


]

---

## Data: NFL teams summary

Created dataset using [`nflfastR`](https://www.nflfastr.com/) summarizing NFL team performances from 1999 to 2021

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
nfl_teams_data <- read_csv("https://shorturl.at/uwAV2")
nfl_model_data <- nfl_teams_data %>%
  mutate(score_diff = points_scored - points_allowed) %>%
  filter(season >= 2006) %>%
  dplyr::select(-wins, -losses, -ties, -points_scored, -points_allowed, -season, -team)
head(nfl_model_data)
```

---

## Using `glmnet`

Divide data into matrix of predictors and vector of response

```{r model-matrix}
model_x <- model.matrix(score_diff ~ ., nfl_model_data)[, -1]
model_y <- nfl_model_data$score_diff
```

Fit linear regression models:

.pull-left[

```{r}
# Base R
init_reg_base <- lm(score_diff ~ ., 
                    nfl_model_data)
```


]

.pull-right[
```{r}
library(glmnet)
init_ridge_glmnet <- glmnet(model_x, model_y, 
                          alpha = 0)
```

]

--

Let's leave all that behind...

```{r}
library(tidymodels)
```


---

## Instead, specify the model with `parsnip`

1. Pick a __model__: in this case, all we need is linear regression, but we could be using e.g. random forests

2. Set the __engine__: which package should we look to for this method?

3. Set the __mode__ (if needed): are we performing classification or regression?

--

This lets us use a similar syntax with many, many different kinds of models by just pointing to their own implementation

According to the tidymodels [website](https://www.tidymodels.org/find/parsnip/), `parsnip` currently supports 119 types of models and engines!

---

## For today...

`linear_reg()` specifies a model that uses linear regression

```{r eval = F}
linear_reg(
  mode = "regression",  # this is the "default" mode, but could change
  engine = "lm",        # default is to use base R linear model
  penalty = NULL,       # default is no penalty term... OLS
  mixture = NULL        # default is no mixture of penalties (patience, grasshopper)
)
```


--
We could also use __pipes__ to specify different arguments here

```{r eval = F}
linear_reg() %>% 
  set_mode(mode = "regression") %>% 
  set_engine(engine = "lm")
```

--
Save this model specification in an object to use later
```{r}
lm_spec <- linear_reg()
```

---

## Train a model based on your specifications: `fit()`

.pull-left[

`fit()` returns a parsnip model fit on the data, using your model form

```{r}
simple_parsnip <- lm_spec %>% 
  fit(    # formula for the model
    score_diff ~ offense_n_plays_run,  
    data = nfl_model_data
  )
simple_parsnip
```

]

.pull-right[

Compare with `lm()` output

```{r}
simple_base <- lm(score_diff ~ 
                    offense_n_plays_run,
   data = nfl_model_data)
simple_base
```

]

---

.pull-left[

## Generate predictions

Just like with a linear model object, we can get predictions using `predict()`

```{r}
preds <- predict(simple_parsnip, 
        new_data = nfl_model_data)
head(preds)
```

But with `parsnip` models, this returns a tibble rather than a vector


]

.pull-right[

## Assess model using RMSE

`yardstick` package has its own `rmse()` function!

```{r}
nfl_lm_mod_assess <- simple_parsnip %>% 
  predict(new_data = nfl_model_data) %>% 
  mutate(obs_score_diff =
           nfl_model_data$score_diff)

rmse(data = nfl_lm_mod_assess,
     truth = obs_score_diff,
     estimate = .pred)

```

]

---

## Which kind of error was that?

--
__Training error!__ We predicted based on the same data we used to fit the model

--

But tidymodels makes it really easy to compare train vs. test error as well...

--
Construct a train-test split using `initial_split()`
```{r}
set.seed(1234)
nfl_split <- initial_split(nfl_model_data, 
                           prop = 0.75)  # 3/4 split is default but could change
nfl_split
```

Recover the training and test sets using `training()` and `testing()`
```{r}
nfl_train <- training(nfl_split)
nfl_test <- testing(nfl_split)
```

---

## Fit and assess based on holdout performance

```{r}
simple_train <- lm_spec %>% 
  fit(score_diff ~ offense_n_plays_run, nfl_train)

predict(simple_train, new_data = nfl_test) %>% 
  mutate(obs_score_diff = nfl_test$score_diff) %>% 
  rmse(truth = obs_score_diff, estimate = .pred)
```
--

Fantastic, we've determined that the test error was greater than the training error.

But this was just for simple linear regression...

---

## Simple $\rightarrow$ multiple linear regression

All we need to do is specify more variables in our formula

```{r}
two_var_parsnip <- lm_spec %>% 
  fit(    # formula for the model
    score_diff ~ offense_n_plays_run + #<<
      offense_n_plays_pass,  #<<
    data = nfl_train
  )

predict(two_var_parsnip, new_data = nfl_test) %>% 
  mutate(obs_score_diff = nfl_test$score_diff) %>% 
  rmse(truth = obs_score_diff, estimate = .pred)
```

--
But this only used one train-test split? What if I want cross-validation?

---

## `vfold_cv()`

Rather than using a simple train-test split, assign observations to one of ten folds

Then, fit the model using `fit_resamples()`

```{r}
set.seed(52)
folds <- vfold_cv(nfl_model_data, v = 10) 
two_var_cv <- lm_spec %>% 
  fit_resamples(score_diff ~ offense_n_plays_run + 
                  offense_n_plays_pass, 
                folds)
```
This gives us a really big model object, with performance metrics on all the folds, which we can view using `collect_metrics()`
```{r}
collect_metrics(two_var_cv)
```

---

## But why tho???

## Didn't we do all this the other day???
--


```{r echo = FALSE, fig.align='center'}
knitr::include_graphics("https://media.tenor.com/Jj6TNPO2w1QAAAAd/stranger-things-eddie-munson.gif")
```

---

## What we did the other day:

With `glmnet`, we could fit lasso and ridge regression models with 10-fold cross-validation

`cv.glmnet` would determine the best value for $\lambda$, given a value for $\alpha$, but if we wanted to use __*elastic net*__ we had to code the CV ourselves:

```{r eval=FALSE}
set.seed(2020)
fold_id <- sample(rep(1:10, length.out = nrow(model_x)))

cv_en_25 <- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = .25)
cv_en_50 <- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = .5)
cv_ridge <- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 0)
cv_lasso <- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 1)

which.min(c(min(cv_en_25$cvm), min(cv_en_50$cvm), min(cv_ridge$cvm), min(cv_lasso$cvm)))
```

--

With `tidymodels`, specifically `tune`, we can do this all in one go!

---

## From multiple linear regression to regularized regression...

Add a penalization term!

Remember our initial model form using `parsnip`? All we need to do is modify the arguments for `engine`, `penalty`, and `mixture`!

```{r}
ridge_example <- linear_reg(
  mode = "regression",  
  engine = "glmnet",        # instead of `lm` change to `glmnet`
  penalty = 0.1,            # set lambda here
  mixture = 0               # "mixture" == "alpha"
)
```

--

`penalty` = "lambda"

`mixture` = "alpha" from yesterday 

--

But you picked those penalty and mixture values yourself!?

---

## `tune` picks the best for you!

```{r}
elastic_net_spec <- linear_reg(
  mode = "regression",  
  engine = "glmnet",
  penalty = tune(),  #<<
  mixture = tune()   #<<      
)
```

`tune()` acts like a placeholder when specifying the hyperparameters for the model

#### Specify values to try using the `dials` package

I want evenly spaced values for the penalty term $\lambda$ and the mixture term $\alpha$, and I want five of each:

```{r}
elnet_grid <- grid_regular(penalty(), mixture(),
                           levels = 5)
```

---

## Model tuning with a grid

First, create the folds as before

```{r}
set.seed(52)
folds <- vfold_cv(nfl_model_data, v = 10)
```

--
Then, fit models using `tune_grid()` to use each combination of penalty and mixture values
```{r}
elnet_resample <- tune_grid(
  elastic_net_spec,
  score_diff ~ .,
  resamples = folds,
  grid = elnet_grid
)
```

---

## Which models (which hyperparameter values) performed best?

.pull-left[
```{r elnet-grid, eval = FALSE}
elnet_resample %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>% 
  mutate(mixture = factor(mixture)) %>%
  ggplot(aes(x = penalty, 
             y = mean, 
             color = mixture)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  scale_x_log10(labels = 
                  scales::label_number()) +
  theme_bw()
```


]

.pull-right[
```{r out.width='90%', ref.label = 'elnet-grid', echo = FALSE}

```


]

---

## Picking the best one...

```{r}
best_elnet <- elnet_resample %>% 
  select_best("rmse") #<<

final_elnet <- linear_reg(
  engine = "glmnet", penalty = best_elnet$penalty, mixture = best_elnet$mixture
) %>% 
  fit(score_diff ~ ., data = nfl_train)
```

So then we can use this model to generate predictions, etc.

--

Note: That last step would have been even easier if we had used a __recipe__ and a __workflow__

---

## Recipes and Workflows

Two more packages within tidymodels that allow you to keep track of and recreate your data wrangling and model specification process

Recipe = `formula` + `data`

- Can specify "roles" for different variables, e.g. tell that a particular column acts as an ID

- Turn a factor variable into dummy variables, e.g. what if we wanted a predictor for each team in the NFL?

--

Workflow = `model` + `recipe`

- Can combine different preprocessing setups (recipes) with different model specs (from parsnip) to compare

- Stores all model results, predictions, etc. in an object where they can be __extracted__

--

All in all, __tidymodels__ allows for great flexibility, while minimizing headaches

