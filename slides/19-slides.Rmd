---
title: "Supervised Learning"
subtitle: "K-Nearest Neighbors Regression and Classification"
date: "June 30th, 2023"
output:
  xaringan::moon_reader:
    lib_dir: "libs"
    # chakra: "libs/remark-latest.min.js"
    # css: ["default", "css/ath-slides.css", "css/ath-inferno-fonts.css", "css/animate.css"]
    self-contained: yes
    # css: [default, default-fonts]
    # seal: false
    # anchor_sections: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      navigation:
        scroll: false
editor_options:
  chunk_output_type: inline
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(tidyverse)
style_mono_accent(base_color = "#cc002b")
```

```{r setup, echo = FALSE}
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
```

## Supervised learning so far...

Linear Regression: Assumptions

--

- The relationship is __linear__

$$Y_{i}=\beta_{0}+\beta_{1} X_{i}+\epsilon_{i}, \quad \text { for } i=1,2, \dots, n$$

$$ \epsilon_i \overset{iid}{\sim}N(0, \sigma^2) \quad \text{ with constant variance } \sigma^2 $$

- Error terms must be __independently, identically distributed__ from Normal distribution (actually three assumptions!)

  - Normality
  
  - Independence
  
  - Homoscedasticity

  
Each of these assumptions must be __checked__

--

Assumptions (in general) make estimation and inference easier but may not always be appropriate

---

## Parametric models

When we make assumptions about distributions, we are saying that we can summarize them with a set of __parameters__

Examples...

--

- Normal distribution: $N(\mu, \sigma^2)$, the __mean__ $\mu$ and __variance__ $\sigma^2$ are parameters

--

- Poisson distribution: $\text{Pois}(\lambda)$, the only parameter is $\lambda$, which is both the __mean and variance__

--

- Simple linear regression: $Y=\beta_{0}+\beta_{1} X$, the coefficients (__intercept__ $\beta_0$ and __slope__ $\beta_1$) are parameters

--

But what if we don't want to make assumptions __a priori__ about our data?


---

## Nonparametric models

Still assume a function mapping the predictors to the response

$$ Y = f(X) $$
And we still want to __estimate__ this regression or classification function

But now we do not assume that this function takes any __particular form__ (e.g. do not assume that the relationship is linear)

--

.pull-left[

### Benefits

- Very flexible, allowing for fitting to very complex relationships

- Often show better performance, more accurate predictions

]

.pull-right[

### Limitations

- Frequently less interpretable

- Prone to overfitting

- Computational concerns (require more data, take longer to train)

]

---

## Regression vs. classification

- __Regression__ models: estimate _average_ value of response

- __Classification__ models: determine _the most likely_ class of a set of discrete response variable classes

Linear (simple or multiple) models $\rightarrow$ regression

Logistic models $\rightarrow$ classification!

--

Examples of classification:

- Binary: given covariates like running yards and completion percentage, can we predict __whether or not__ a QB is likely to be a Hall-of-Famer?

- Multi-class: from predictors like tumor size, location, and cell types, can we determine __which kind__ of cancer a patient has?

--

Question of interest determines which type of model to use

Many nonparametric methods (e.g. KNN) can be used either for classification __OR__ regression, just change the final outcome

---

## Setup: Binary classification problem

.pull-left[

$Y = f(X_1, X_2)$

$Y \in \{0, 1\}$

How might we model $f(X_1, X_2)$?

__"Decision boundary"__

Example settings:

- QB's running and passing yards $(X_1, X_2)$, HOF status $(Y)$

- Patient's length of inpatient rehab stay and report of outpatient stress $(X_1, X_2)$, substance abuse relapse $(Y)$

]


.pull_right[

```{r out.width='50%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRLpgBIfLPTFlVsF32It6Ues8tZNDzAYFlMOA&usqp=CAU")
```

]

---

## K-Nearest Neighbors Classification

.pull-left[

Classify a __new point__ based on a majority vote of the __k__ points closest to it in space

__Close?__ Use Euclidean distance

$$d(x_i, x_j) = \sqrt{(x_{i1}-x_{j1})^2 + \cdots + (x_{ip}-x_{jp})^2}$$

Have to choose how many neighbors to query: which value of __k__ to pick

]


.pull_right[

```{r out.width='50%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRLpgBIfLPTFlVsF32It6Ues8tZNDzAYFlMOA&usqp=CAU")
```

]

---

### Different values of k lead to different decision boundaries

```{r out.width='100%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://elvyna.github.io/images/posts/2019-01-28-knn-explained/6-k-affects-decision-boundary.png?style=centerme")
```

--

Excellent illustration of the __bias-variance tradeoff__

--

As __k increases__, the __variance decreases__ (different training samples will produce more similar decision boundaries), but the __bias increases__ (the decision boundary gets further from the truth)

---

## Picking the number of neighbors

The number of neighbors __k__ is a __hyperparameter__

Tune this value using a train-test split or using cross-validation

__Note__: 1NN classification will have zero training error... __why__ ?

--

```{r out.width='30%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://kevinzakka.github.io/assets/knn/teaser.png")
```

--

#### Ideally: balance the flexibility, allowing for a complex decision boundary, __but__ don't fit to the noise

---

## Data: NFL field goal attempts

Created dataset using [`nflscrapR-data`](https://github.com/ryurko/nflscrapR-data) of all NFL field goal attempts from 2009 to 2019

```{r, warning = FALSE, message = FALSE}
nfl_fg_attempts <- read_csv("https://shorturl.at/mCGN2") %>% 
  filter(pbp_season == 2014) %>% 
  mutate(is_fg_made = as_factor(is_fg_made))
head(nfl_fg_attempts)
```

__Response__: is the field goal made?

---

## Predicting field goals

.pull-left[

Explanatory variables: 

Kick distance and score differential


```{r basic-plot, eval = FALSE}
ggplot(nfl_fg_attempts) +
  geom_point(aes(x = kick_distance, 
                 y = score_differential,
                 color = is_fg_made), 
             size = 0.7) +
  theme_bw()
```

Does there appear to be a relationship between these predictors and the success of field goal attempts?

]

.pull-right[
```{r, ref.label='basic-plot', echo = FALSE,  fig.height=6,fig.width=6,fig.align="center"}

```
]

---

## KNN classification

Using the [FNN package](https://rdrr.io/cran/FNN/) (fast implementation of KNN models)

Separate predictors from the response column:

```{r}
fg_x <- dplyr::select(nfl_fg_attempts, kick_distance, score_differential)
fg_y <- nfl_fg_attempts$is_fg_made
```

Give `knn()` 

- a training set with all explanatory columns `train`

- a "test" set including all explanatory columns `test`

  - if you provide the same set for both, it will give predictions on the training set

- a vector of true classifications for the training set `cl`

- a value for number of nearest neighbors `k`

- instructions for which `algorithm` to use ("brute" means that it will do a brute-force search, but it can also do tree-based searches)

---

## Fitting a KNN classifier

Trying $k=1$:

```{r}
library(FNN)

init_knn <- knn(train = fg_x, test = fg_x, cl = fg_y, k = 1, algorithm = "brute")
```

This outputs a vector of predicted classes for the "test" set (in this case, predictions on the training set)

--

How well does this model perform?
```{r}
mean(nfl_fg_attempts$is_fg_made == init_knn)
```

__92%__ But we said that 1NN would achieve zero training error?

--

In this dataset, there are multiple observations for some distance-differential combinations, which might have both made and missed field goals, so it then takes the majority vote among those "equivalent" points (see the documentation!)

---

## Training vs. test

But, as we know, we should not assess models based on training error, we need some measure of holdout performance:

Split the data, train the model, then assess performance on the test set

(Yet another way to manually code a train-test split)
```{r}
set.seed(50)
train_ids <- sample(1:nrow(nfl_fg_attempts), 
                    ceiling(0.75 * nrow(nfl_fg_attempts)))

train_nfl <- nfl_fg_attempts[train_ids, ]
test_nfl <- nfl_fg_attempts[-train_ids, ]

# separate predictors and response, again
train_x <- dplyr::select(train_nfl, kick_distance, score_differential)
train_y <- train_nfl$is_fg_made
test_x <- dplyr::select(test_nfl, kick_distance, score_differential)
test_y <- test_nfl$is_fg_made
```

---

## Assessing 1NN

1 Nearest Neighbor classifier: How well does it perform on the training data?
```{r}
one_nn_train_preds <- knn(train = train_x, test = train_x, cl = train_y, k = 1, algorithm = "brute")
mean(train_y == one_nn_train_preds)
```
--

... But what about on the test data?

```{r}
one_nn_test_preds <- knn(train = train_x, test = test_x, cl = train_y, k = 1, algorithm = "brute")
mean(test_y == one_nn_test_preds)
```
Performance decreases! We're overfitting!

---

## Which value of k to pick?

As a demonstration, let's loop through possible values of k and see which ends up having the best holdout performance

```{r}
errs_train <- rep(0, 12)
errs_test <- rep(0, 12)
k_vals <- 1:12

for(k in k_vals) {
  
  train_preds <- knn(train = train_x, test = train_x, cl = train_y, k = k, algorithm = "brute")
  errs_train[k] <- mean(!train_y == train_preds)
  
  test_preds <- knn(train = train_x, test = test_x, cl = train_y, k = k, algorithm = "brute")
  errs_test[k] <- mean(!test_y == test_preds)
}
```

--

Side note: beyond k = 12, the model predicts success for the entire test set

---

## Plot training and test error

.pull-left[

```{r err-plot, eval = FALSE}
errors <- bind_cols(train_err = errs_train,
                    test_err = errs_test,
                    k = k_vals)

errors %>% 
  pivot_longer(c(train_err, test_err), 
               names_to = "err_type") %>% 
  ggplot(aes(x = k, y = value, 
             color = err_type)) +
  geom_point() +
  geom_line() +
  labs(y = "Misclassification Rate", 
       color = "Type of error") +
  theme_bw()
```


]

.pull-right[
```{r, ref.label='err-plot', echo = FALSE,  fig.height=6,fig.width=6,fig.align="center"}

```
]

---

## But wait, there's more!

The k-nearest-neighbors algorithm can also be used for __regression__!

As before:

- Find the k points closest to your new observation (based on Euclidean distance)

--

But now,

- Rather than taking a _majority vote_ to determine a class, take the __average response__ among the neighbors

- Predict this average value as the response for the new point: 


$$\hat{f}(x^*) = \text{Ave}(y_i | x_i \in N_k(x^*))$$

--

(__Side Note__: You can also do cool stuff like weighting those responses based on how close they are to the new point! Lots of modifications to knn regression out there)

---

## Brief KNN Regression Example

.pull-left[

Gapminder data again

```{r lm-plot, eval=FALSE}
library(dslabs)
clean_gapminder <- as_tibble(gapminder) %>%
  filter(year == 2011, !is.na(gdp)) %>%
  mutate(log_gdp = log(gdp))

gdp_plot <- clean_gapminder %>%
  ggplot(aes(x = log_gdp,
             y = life_expectancy)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") + #<<
  theme_bw() +
  labs(x = "log(GDP)",
       y = "Life expectancy")
gdp_plot
```

]

.pull-right[
```{r, ref.label='lm-plot', echo = FALSE,  fig.height=6,fig.width=6,fig.align="center"}

```

]

---

## KNN Regression for Life Expectancy

.pull-left[
```{r knn-reg-plot, eval=FALSE}
k2_le <- knn.reg(
  train = clean_gapminder$log_gdp,
  y = clean_gapminder$life_expectancy,
  k = 2)
k5_le <- knn.reg(
  train = clean_gapminder$log_gdp,
  y = clean_gapminder$life_expectancy,
  k = 5)
k15_le <- knn.reg(
  train = clean_gapminder$log_gdp,
  y = clean_gapminder$life_expectancy,
  k = 15)

gdp_plot +
  geom_line(aes(x = log_gdp, y = k2_le$pred), #<<
            color = "darkgreen") +
  geom_line(aes(x = log_gdp, y = k5_le$pred), #<<
            color = "darkred") +
  geom_line(aes(x = log_gdp, y = k15_le$pred), #<<
            color = "purple")
```


]

.pull-right[
```{r, ref.label='knn-reg-plot', echo = FALSE,  fig.height=6,fig.width=6,fig.align="center"}

```

]

---

## KNN with tidymodels

Of course, we can also use the tidymodels framework to fit and use a KNN regression model:

```{r}
library(tidymodels)
knn_reg_mod <- nearest_neighbor(
  mode = "regression",
  engine = "kknn",
  neighbors = 5,         # we could also tune this with cross-validation
  weight_func = NULL,    # for weighted KNN!
  dist_power = NULL      # for use with a different distance metric
)

gapminder_knn_fit <- knn_reg_mod %>% 
  fit(
    life_expectancy ~ log_gdp,
    data = clean_gapminder
  )
```
