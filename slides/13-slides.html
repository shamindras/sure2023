<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Supervised Learning</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Supervised Learning
]
.subtitle[
## Linear regression
]
.date[
### June 21st, 2021
]

---






## Simple linear regression

We assume a __linear relationship__ for `\(Y = f(X)\)`:

`$$Y_{i}=\beta_{0}+\beta_{1} X_{i}+\epsilon_{i}, \quad \text { for } i=1,2, \dots, n$$`

- `\(Y_i\)` is the `\(i\)`th value for the __response__ variable
  
- `\(X_i\)` is the `\(i\)`th value for the __predictor__ variable

--
  
- `\(\beta_0\)` is an _unknown_, constant __intercept__: average value for `\(Y\)` if `\(X = 0\)`
  
- `\(\beta_1\)` is an _unknown_, constant __slope__: increase in average value for `\(Y\)` for each one-unit increase in `\(X\)`
  
--
  
- `\(\epsilon_i\)` is the __random__ noise: assume __independent, identically distributed__ (_iid_) from Normal distribution

`$$\epsilon_i \overset{iid}{\sim}N(0, \sigma^2) \quad \text{ with constant variance } \sigma^2$$`

---

## Simple linear regression estimation

We are estimating the __conditional expection (mean)__ for `\(Y\)`:

`$$\mathbb{E}[Y_i| X_i] = \beta_0 + \beta_1X_i$$`

- average value for `\(Y\)` given the value for `\(X\)`

--

How do we estimate the __best fitting__ line?

--

__Ordinary least squares (OLS)__ - by minimizing the __residual sum of squares (RSS)__

`$$R S S\left(\beta_{0}, \beta_{1}\right)=\sum_{i=1}^{n}\left[Y_{i}-\left(\beta_{0}+\beta_{1} X_{i}\right)\right]^{2}=\sum_{i=1}^{n}\left(Y_{i}-\beta_{0}-\beta_{1} X_{i}\right)^{2}$$`

--

`$$\widehat{\beta}_{1}=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}} \quad \text{ and } \quad \widehat{\beta}_{0}=\bar{Y}-\widehat{\beta}_{1} \bar{X}$$`

where `\(\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i\)` and `\(\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i\)`

---

## Connection to covariance and correlation

[__Covariance__](https://en.wikipedia.org/wiki/Covariance) describes the __joint variability of two variables__

`$$\text{Cov}(X, Y) = \sigma_{X,Y} = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]$$`

--

We compute the __sample covariance__ (use `\(n - 1\)` since we are using the means and want [__unbiased estimates__](https://lazyprogrammer.me/covariance-matrix-divide-by-n-or-n-1/))

`$$\hat{\sigma}_{X,Y} = \frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)$$`

--

__Correlation__ is a _normalized_ form of covariance, ranges from -1 to 1

`$$\rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sigma_X \cdot \sigma_Y}$$`

__Sample correlation__ uses the sample covariance and standard deviations, e.g. `\(s_X^2 = \frac{1}{n-1} \sum_i (X_i - \bar{X})^2\)`

`$$r_{X,Y} = \frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}$$`

---

## Connection to covariance and correlation

So we have the following:

`$$\widehat{\beta}_{1}=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}} \quad \text{ compared to } \quad r_{X,Y} = \frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}$$`

--

`\(\Rightarrow\)` Can rewrite `\(\hat{\beta}_1\)` as:

`$$\widehat{\beta}_{1} = r_{X,Y} \cdot \frac{s_Y}{s_X}$$`

`\(\Rightarrow\)` Can rewrite `\(r_{X,Y}\)` as:

`$$r_{X,Y} = \widehat{\beta}_{1} \cdot \frac{s_X}{s_Y}$$`

--

Can think of `\(\widehat{\beta}_{1}\)` weighting the ratio of variance between `\(X\)` and `\(Y\)`...

---

## Example data: NFL teams summary

Created dataset using [`nflfastR`](https://www.nflfastr.com/) summarizing NFL team performances from 1999 to 2020


```r
library(tidyverse)
nfl_teams_data &lt;- read_csv("http://www.stat.cmu.edu/cmsac/sure/2021/materials/data/regression_projects/nfl_team_season_summary.csv")
nfl_teams_data
```

```
## # A tibble: 701 × 55
##    season team  offens…¹ offen…² offen…³ offen…⁴ offen…⁵ offen…⁶ offen…⁷ offen…⁸
##     &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1   1999 ARI      0.477    2796    1209    4.67    3.15       0     NaN       0
##  2   1999 ATL      0.504    3317    1176    6.08    3.20       0     NaN      11
##  3   1999 BAL      0.452    2805    1663    5.07    4.13       0     NaN       0
##  4   1999 BUF      0.540    3275    2038    6.17    4.13       0     NaN     161
##  5   1999 CAR      0.552    4144    1484    6.68    4.29       0     NaN      89
##  6   1999 CHI      0.561    4090    1359    5.75    3.55       0     NaN     508
##  7   1999 CIN      0.498    3178    1971    5.37    4.63       0     NaN       0
##  8   1999 CLE      0.489    2574    1140    4.71    3.67       0     NaN      35
##  9   1999 DAL      0.560    3083    2054    5.95    4.29       0     NaN       0
## 10   1999 DEN      0.546    3378    1852    5.85    4.05       0     NaN       9
## # … with 691 more rows, 45 more variables: offense_ave_yac &lt;dbl&gt;,
## #   offense_n_plays_pass &lt;dbl&gt;, offense_n_plays_run &lt;dbl&gt;,
## #   offense_n_interceptions &lt;dbl&gt;, offense_n_fumbles_lost_pass &lt;dbl&gt;,
## #   offense_n_fumbles_lost_run &lt;dbl&gt;, offense_total_epa_pass &lt;dbl&gt;,
## #   offense_total_epa_run &lt;dbl&gt;, offense_ave_epa_pass &lt;dbl&gt;,
## #   offense_ave_epa_run &lt;dbl&gt;, offense_total_wpa_pass &lt;dbl&gt;,
## #   offense_total_wpa_run &lt;dbl&gt;, offense_ave_wpa_pass &lt;dbl&gt;, …
```

---

## Modeling NFL score differential

.pull-left[

Interested in modeling a team's __score differential__


```r
nfl_teams_data &lt;- nfl_teams_data %&gt;%
  mutate(score_diff = 
           points_scored - points_allowed)
nfl_teams_data %&gt;%
  ggplot(aes(x = score_diff)) +
  geom_histogram(color = "black", 
                 fill = "darkblue",
                 alpha = 0.3) +
  theme_bw() +
  labs(x = "Score differential")
```



]
.pull-right[

&lt;img src="13-slides_files/figure-html/unnamed-chunk-2-1.png" width="504" /&gt;


]

---

### Relationship between score differential and passing performance

.pull-left[
- `offense_ave_epa_pass`: team's average [expected points added (EPA)]() per pass attempt


```r
nfl_plot &lt;- nfl_teams_data %&gt;%
* ggplot(aes(x = offense_ave_epa_pass,
             y = score_diff)) +
  geom_point(alpha = 0.5) +
  theme_bw() +
  labs(x = "EPA gained per pass attempt",
       y = "Score differential")
nfl_plot
```

We fit linear regression models using `lm()`, formula is input as: `response ~ predictor`


```r
*init_lm &lt;- lm(score_diff ~ offense_ave_epa_pass,
*             data = nfl_teams_data)
```


]
.pull-right[
&lt;img src="13-slides_files/figure-html/unnamed-chunk-3-1.png" width="504" /&gt;
]

---

## View the model `summary()`


```r
summary(init_lm)
```

```
## 
## Call:
## lm(formula = score_diff ~ offense_ave_epa_pass, data = nfl_teams_data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -182.611  -46.600   -1.326   44.123  233.640 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)            -4.890      2.561  -1.909   0.0566 .  
## offense_ave_epa_pass  566.352     19.226  29.458   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 67.67 on 699 degrees of freedom
## Multiple R-squared:  0.5539,	Adjusted R-squared:  0.5532 
## F-statistic: 867.8 on 1 and 699 DF,  p-value: &lt; 2.2e-16
```

---

## Inference with OLS

Reports the intercept and coefficient estimates: `\(\quad \hat{\beta}_0 \approx -4.89 \quad, \quad \hat{\beta}_1 \approx 566.352\)` 

--

Estimates of uncertainty for `\(\beta\)`s via __standard errors__: `\(\quad \widehat{SE}(\hat{\beta}_0) \approx 2.561 \quad, \quad \widehat{SE}(\hat{\beta}_1) \approx 19.226\)` 

--

`\(t\)`-statistics are coefficients `Estimates` / `Std. Error`, i.e., number of standard deviations from 0

  - _p-values_ (i.e., `Pr(&gt;|t|)`): estimated probability observing value as extreme as |`t value`| __given the null hypothesis__ `\(\beta = 0\)`
  
  - p-value `\(&lt;\)` conventional threshold of `\(\alpha = 0.05\)`, __sufficient evidence to reject the null hypothesis that the coefficient is zero__,
  
  - Typically |`t values`| `\(&gt; 2\)` indicate __significant__ relationship at `\(\alpha = 0.05\)`

  - i.e., there is a __significant__ association between `offense_ave_epa_pass` and `score_diff`


---

## Be careful!

__Caveats to keep in mind regarding p-values:__

--

If the true value of a coefficient `\(\beta = 0\)`, then the p-value is sampled from a [Uniform(0,1) distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution)

- i.e. it is just as likely to have value 0.45 as 0.16 or 0.84 or 0.9999 or 0.00001...

--

`\(\Rightarrow\)` Hence why we typically only reject for low `\(\alpha\)` values like 0.05

  - Controlling the Type 1 error rate at `\(\alpha = 0.05\)`, i.e., the probability of a __false positive__ mistake
  
  - 5% chance that you'll conclude there's a significant association between `\(x\)` and `\(y\)` __even when there is none__

--

Remember what a standard error is? `\(SE = \frac{\sigma}{\sqrt{n}}\)`

  - `\(\Rightarrow\)` As `\(n\)` gets large __standard error goes to zero__, and *all* predictors are eventually deemed significant

- While the p-values might be informative, we will explore other approaches to determine which subset of predictors to include (e.g., holdout performance)

---

## Back to the model summary: `Multiple R-squared`

Back to the connection between the coefficient and correlation:

`$$r_{X,Y} = \widehat{\beta}_{1} \cdot \frac{s_X}{s_Y} \quad \Rightarrow \quad r^2_{X,Y} = \widehat{\beta}_{1}^2\cdot \frac{s_X^2}{s_Y^2}$$`

Compute the correlation with `cor()`:


```r
*with(nfl_teams_data, cor(offense_ave_epa_pass, score_diff))
```

```
## [1] 0.7442135
```

--

The squared `cor` matches the reported `Multiple R-squared`


```r
*with(nfl_teams_data, cor(offense_ave_epa_pass, score_diff))^2
```

```
## [1] 0.5538537
```


---

## Back to the model summary: `Multiple R-squared`

Back to the connection between the coefficient and correlation:

`$$r_{X,Y} = \widehat{\beta}_{1} \cdot \frac{s_X}{s_Y} \quad \Rightarrow \quad r^2_{X,Y} = \widehat{\beta}_{1}^2\cdot \frac{s_X^2}{s_Y^2}$$`

`\(r^2\)` (or also `\(R^2\)`) estimates the __proportion of the variance__ of `\(Y\)` explained by `\(X\)` 

  - More generally: variance of model predictions / variance of `\(Y\)`


```r
*var(predict(init_lm)) / var(nfl_teams_data$score_diff)
```

```
## [1] 0.5538537
```


---

## Generating predictions

We can use the `predict()` function to either get the fitted values of the regression:


```r
train_preds &lt;- predict(init_lm)
head(train_preds)
```

```
##          1          2          3          4          5          6 
## -120.21628  -33.58829 -104.31202   21.15045   57.18906  -23.34489
```

Which is equivalent to using:

```r
head(init_lm$fitted.values)
```

```
##          1          2          3          4          5          6 
## -120.21628  -33.58829 -104.31202   21.15045   57.18906  -23.34489
```


---

## Predictions for new data

Or we can provide it `newdata` which __must contain the explanatory variables__:

.pull-left[

```r
steelers_data &lt;- nfl_teams_data %&gt;% 
  filter(team == "PIT", season == 2020)

new_steelers_data &lt;- steelers_data %&gt;%
  dplyr::select(team, offense_ave_epa_pass) %&gt;%
* slice(rep(1, 3)) %&gt;%
  mutate(adj_factor = c(0.5, 1, 2),
         offense_ave_epa_pass = offense_ave_epa_pass * adj_factor)
new_steelers_data$pred_score_diff &lt;- 
* predict(init_lm, newdata = new_steelers_data)
nfl_plot +
  geom_point(data = new_steelers_data,
             aes(x = offense_ave_epa_pass,
                 y = pred_score_diff),
             color = "gold", size = 5)
```
]
.pull-right[
&lt;img src="13-slides_files/figure-html/unnamed-chunk-4-1.png" width="432" style="display: block; margin: auto;" /&gt;


]

---

### Plot observed values against predictions

Useful diagnostic (for __any type of model__, not just linear regression!) 

.pull-left[


```r
nfl_teams_data %&gt;%
* mutate(pred_vals = predict(init_lm)) %&gt;%
  ggplot(aes(x = pred_vals,
             y = score_diff)) +
  geom_point(alpha = 0.5) +
* geom_abline(slope = 1, intercept = 0,
              linetype = "dashed",
              color = "red",
              size = 2) +
  theme_bw()
```

- "Perfect" model will follow __diagonal__

]
.pull-right[
&lt;img src="13-slides_files/figure-html/unnamed-chunk-5-1.png" width="432" style="display: block; margin: auto;" /&gt;


]

---

### Plot observed values against predictions

Can augment the data with model output using the [`broom` package](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)

.pull-left[


```r
nfl_teams_data &lt;- 
* broom::augment(init_lm, nfl_teams_data)
nfl_teams_data %&gt;%
  ggplot(aes(x = .fitted, 
*            y = score_diff)) +
  geom_point(alpha = 0.5) +
* geom_abline(slope = 1, intercept = 0,
              linetype = "dashed",
              color = "red",
              size = 2) +
  theme_bw()
```

- Adds various columns from model fit we can use in plotting for model diagnostics

]
.pull-right[
&lt;img src="13-slides_files/figure-html/unnamed-chunk-6-1.png" width="432" style="display: block; margin: auto;" /&gt;


]

---

## Plot residuals against predicted values

.pull-left[

- Residuals = observed - predicted

- Conditional on the predicted values, the __residuals should have a mean of zero__


```r
nfl_teams_data %&gt;%
  ggplot(aes(x = .fitted, 
*            y = .resid)) +
  geom_point(alpha = 0.5) +
* geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "red",
             size = 2) +
  # To plot the residual mean
* geom_smooth(se = FALSE) +
  theme_bw()
```

- Residuals __should NOT display any pattern__

]
.pull-right[
&lt;img src="13-slides_files/figure-html/unnamed-chunk-7-1.png" width="432" style="display: block; margin: auto;" /&gt;


]

---

## Multiple regression 

We can include as many variables as we want (assuming `\(n &gt; p\)`!)

`$$Y=\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}+\epsilon$$`

OLS estimates in matrix notation ( `\(\boldsymbol{X}\)` is a `\(n \times p\)` matrix):
  
`$$\hat{\boldsymbol{\beta}} = (\boldsymbol{X} ^T \boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}$$`

--

Can just add more variables to the formula in `R`


```r
*multiple_lm &lt;- lm(score_diff ~ offense_ave_epa_pass + defense_ave_epa_pass,
                  data = nfl_teams_data)
```

- Use the `Adjusted R-squared` when including multiple variables `\(= 1 - \frac{(1 - R^2)(n - 1)}{(n - p - 1)}\)`

  - Adjusts for the number of variables in the model `\(p\)`  
  
  - Adding more variables __will always increase__ `Multiple R-squared`

---

### What about the Normal distribution assumption???

`$$Y=\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}+\epsilon$$`

- `\(\epsilon_i\)` is the __random__ noise: assume __independent, identically distributed__ (_iid_) from Normal distribution

`$$\epsilon_i \overset{iid}{\sim}N(0, \sigma^2) \quad \text{ with constant variance } \sigma^2$$`

--

__OLS doesn't care about this assumption__, it's just estimating coefficients!

--

In order to perform inference, __we need to impose additional assumptions__

By assuming `\(\epsilon_i \overset{iid}{\sim}N(0, \sigma^2)\)`, what we really mean is:

`$$Y \overset{iid}{\sim}N(\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}, \sigma^2)$$`

--

So we're estimating the mean `\(\mu\)` of this conditional distribution, but what about `\(\sigma^2\)`?
  
--

[Unbiased estimate](https://bradleyboehmke.github.io/HOML/linear-regression.html#simple-linear-regression) `\(\hat{\sigma}^2 = \frac{RSS}{n - (p + 1)}\)`, its square root is the `Residual standard error` 

  - __Degrees of freedom__:  `\(n - (p + 1)\)`, data supplies us with `\(n\)` "degrees of freedom" and we used up `\(p + 1\)`

---

### Check the assumptions about normality with [`ggfortify`](https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_lm.html)


```r
library(ggfortify)
*autoplot(multiple_lm, ncol = 4) + theme_bw()
```

&lt;img src="13-slides_files/figure-html/more-resid-plots-1.png" width="1080" style="display: block; margin: auto;" /&gt;

- Standardized residuals = residuals `/ sd(`residuals`)` (see also `.std.resid` from `augment`)


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
